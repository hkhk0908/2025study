import numpy as np
import sounddevice as sd
import matplotlib.pyplot as plt
from scipy.fft import rfft
import librosa
from tensorflow.keras.models import load_model
import queue
import matplotlib
matplotlib.rc('font', family='Malgun Gothic')  # í•œê¸€ ê¹¨ì§ ë°©ì§€ (Windows ê¸°ì¤€)

# ------------------------------
# ì„¤ì •
# ------------------------------
fs = 16000           # ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz)
duration = 1.0       # ì²˜ë¦¬ ë‹¨ìœ„ ì‹œê°„ (ì´ˆ)
blocksize = int(fs * duration)
model_path = "scream_cnn_model.keras"  # ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ

# ------------------------------
# ëª¨ë¸ ë¡œë“œ
# ------------------------------
model = load_model(model_path)

# ------------------------------
# ì˜¤ë””ì˜¤ í ì„¤ì •
# ------------------------------
q = queue.Queue()

def audio_callback(indata, frames, time, status):
    if status:
        print(status)
    q.put(indata.copy())

# ------------------------------
# MFCC ì¶”ì¶œ í•¨ìˆ˜
# ------------------------------
def extract_mfcc(audio_data, sr=16000, n_mfcc=40):
    audio_data = audio_data.flatten()
    mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc)
    mfcc = mfcc.T
    if mfcc.shape[0] < 40:
        return None
    mfcc = mfcc[:40]
    return mfcc[..., np.newaxis]

# ------------------------------
# FFT ì‹œê°í™” ì´ˆê¸° ì„¤ì •
# ------------------------------
plt.ion()
fig, ax = plt.subplots()
x_fft = np.fft.rfftfreq(blocksize, d=1/fs)
line, = ax.plot(x_fft, np.zeros_like(x_fft))
ax.set_ylim(0, 1000)
ax.set_xlim(0, 4000)
ax.set_xlabel("Frequency (Hz)")
ax.set_ylabel("Amplitude")
ax.set_title("ì‹¤ì‹œê°„ FFT + ë¹„ëª… ê°ì§€")

# ------------------------------
# ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ------------------------------
with sd.InputStream(channels=1, samplerate=fs, callback=audio_callback, blocksize=blocksize):
    print("ğŸ¤ ì‹¤ì‹œê°„ ë¹„ëª… ê°ì§€ ì‹œì‘! (Ctrl+Cë¡œ ì¢…ë£Œ)\n")

    while True:
        audio_block = q.get()
        audio_np = audio_block[:, 0]

        # ì‹¤ì‹œê°„ FFT ê³„ì‚° ë° ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        fft_result = np.abs(rfft(audio_np))
        line.set_ydata(fft_result)
        fig.canvas.draw()
        fig.canvas.flush_events()

        # ëª¨ë¸ ì…ë ¥ ì „ì²˜ë¦¬ (MFCC)
        mfcc = extract_mfcc(audio_np)
        if mfcc is not None:
            mfcc_input = np.expand_dims(mfcc, axis=0)  # (1, 40, 40, 1)
            pred = model.predict(mfcc_input)[0][0]
            print(f"ì˜ˆì¸¡ í™•ë¥ : {pred:.2f}")
            if pred > 0.8:
                print("ğŸš¨ ë¹„ëª… ê°ì§€ë¨!")
